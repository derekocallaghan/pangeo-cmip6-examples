{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPCP Rainfall Frequency Analysis with Xhistogram\n",
    "\n",
    "The goal here is to demonstrate how simple it is to calculate probability distributions from xarray data using [xhistogram](https://xhistogram.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NUMPY_EXPERIMENTAL_ARRAY_FUNCTION'] = '0'\n",
    "\n",
    "import intake\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from xhistogram.xarray import histogram\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  \n",
    "plt.rcParams['figure.figsize'] = 12, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Calculation\n",
    "\n",
    "We are basically trying to reproduce a calculation by [Angie Pendegrass](https://staff.ucar.edu/users/apgrass) described on the NCAR website: <https://climatedataguide.ucar.edu/climate-data/gpcp-daily-global-precipitation-climatology-project>\n",
    "\n",
    "![gpcp_histogram](https://climatedataguide.ucar.edu/sites/default/files/styles/node_key_figures_display/public/key_figures/climate_data_set/cdgbutterflygpcp.jpg?itok=RnJ_w-ge)\n",
    "\n",
    "> Rain frequency distribution at each latitude in GPCP 1DD data, 1997-2013. Left: dry day frequency (%). Right: Rain frequency with logarithmically-spaced rain rate bins. Units are percent per increment of rain rate, so evenly spaced increments of rain rate can be compared. Abrupt transitions between 40 and 50 N are features in the product which occur due to the transition in data sources across this latitude band.\n",
    "\n",
    "Angie offered some clarification about how it was produced in a personal communcation:\n",
    "\n",
    "> For these I cacluated the distribution at each point and then took the zonal mean. I have some python code for how I calculate distributions of precip here: https://github.com/apendergrass/rain-metrics-python - `raindistdemo.py` is the relevant one, it calculates pointwise distributions, and then you can take the zonal mean... The units are set up such that when you sum up the whole histogram it gives you the fraction of days with nonzero precip. If you also add the dry day fraction, it gets you to 100%. This entails normalizing by the spacing of the bins.\n",
    "\n",
    "For reference, [`raindistdemo.py`](https://github.com/apendergrass/rain-metrics-python/blob/master/raindistdemo.py) has about 100 lines of code (excluding comments) and is written in what I would call MATLAB-style python. One goal of this demo is to illustrate how such calculations can be a lot simpler using high-level python tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "First we load the GPCP data from the [Pangeo cloud datastore](https://pangeo-data.github.io/pangeo-datastore/). Note that it uses \"lazy loading,\" represeting the data as dask arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = 'https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/master.yaml'\n",
    "master_cat = intake.Catalog(cat_url)\n",
    "gpcp = master_cat.atmosphere.gpcp_cdr_daily_v1_3.to_dask()\n",
    "gpcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a nice visual summary of the underlying dask arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp.precip.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the Calculation\n",
    "\n",
    "### Define the Bins\n",
    "\n",
    "It's important to note the units of rainfall in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp.precip.units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angie's histogram used logarithmically spaced bins, plus a \"special\" bin for dry days. Since there is no such thing as zero in floating point data, we will define a bin with range `[0, 0.01]` as our \"dry\" bin. We can create such bins as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the units here are mm/day\n",
    "nbins = 100\n",
    "bins = np.hstack([[0], np.logspace(-2, 2, nbins)])\n",
    "bins[:4] # peek at the first four bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also be important to define the bin spacing in log space, since this is how Angie normalized her values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_spacing = np.diff(np.log(bins))\n",
    "bin_spacing[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first special bin, the bins are spaced evenly in log space, as `np.logspace` is supposed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram 3 Ways\n",
    "\n",
    "Calculating the histogram with xhistogram is really easy. In the example below, we calculate the \"raw\" histogram (no normalization) over the time and longitude dimensions, leaving the latitude dimension intact. (The time here includes loading the data over the network, plus computing time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time gpcp_hist = histogram(gpcp.precip, bins=[bins], dim=['time', 'longitude']).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that code, we just counted the number of points in the array falling in each bin.\n",
    "\n",
    "But is that what Angie did? In her words:\n",
    "\n",
    "> For these I cacluated the distribution [over time] at each point and then took the zonal mean\n",
    "\n",
    "That would be more like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time gpcp_hist_time = histogram(gpcp.precip, bins=[bins], dim=['time']).mean(dim='longitude').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a third possibility: take the histogram over longitude and then take the mean in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time gpcp_hist_lon = histogram(gpcp.precip, bins=[bins], dim=['longitude']).mean(dim='time').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That one was actually the fastest, because it aligns most easily with the chunks of the dask array.\n",
    "\n",
    "Do they give the same answer? Let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(16, 4))\n",
    "\n",
    "# the indexing [:, 1:] skips the \"dry bin\"\n",
    "gpcp_hist[:, 1:].plot(ax=axes[0], xscale='log')\n",
    "gpcp_hist_time[:, 1:].plot(ax=axes[1], xscale='log')\n",
    "gpcp_hist_lon[:, 1:].plot(ax=axes[2], xscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear from the visualization that the only difference here is a normalization factor. The first case is a sum over *all* the bins. The second is a sum over bins in time and a mean in longitude, while the third is a sum over bins in longitude and a mean in time. In each case, all the rows sum to the total number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gpcp_hist.sum(dim='precip_bin') / (gpcp.dims['longitude'] * gpcp.dims['time'])).plot()\n",
    "(gpcp_hist_lon.sum(dim='precip_bin') / (gpcp.dims['longitude'])).plot()\n",
    "(gpcp_hist_time.sum(dim='precip_bin') / (gpcp.dims['time'])).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These don't quite all equal one because our bins evidently did not cover all values that appear in the dataset. But it's good enough.\n",
    "\n",
    "Since we verified they all give the same answer, we are free to use the most efficient version:\n",
    "\n",
    "    gpcp_hist_lon = histogram(gpcp.precip, bins=[bins], dim=['longitude']).mean(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "\n",
    "> The units are set up such that when you sum up the whole histogram it gives you the fraction of days with nonzero precip. If you also add the dry day fraction, it gets you to 100%. This entails normalizing by the spacing of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints = gpcp.dims['longitude']\n",
    "\n",
    "gpcp_hist_norm = gpcp_hist_lon[:, 1:] * 100 / (npoints * bin_spacing[1])\n",
    "gpcp_hist_norm.attrs.update({'long_name': 'GPCP 1DD zonal mean rain frequency',\n",
    "                             'units': '%/Î”lnr'})\n",
    "\n",
    "gpcp_dry_norm = gpcp_hist_lon[:, 0] * 100 / npoints\n",
    "gpcp_dry_norm.attrs.update({'long_name': 'r=0',\n",
    "                             'units': '%'})\n",
    "gpcp_hist_norm.plot(xscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the same order of magnitude as Angie's figure.\n",
    "\n",
    "## Reproducing the Plot\n",
    "\n",
    "In order to reproduce the plot exactly, we need a few more details, such as\n",
    "\n",
    "- sine(latitude) y scale\n",
    "- pretty subplots\n",
    "\n",
    "To do the sine latitude scaling, I've done it the \"right\" way, by creating a custom scale object. This unfortunately requires some complex custom code which I basically just copied and modified slightly from the matplotlib documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://matplotlib.org/gallery/api/custom_scale_example.html\n",
    "\n",
    "from matplotlib import scale as mscale\n",
    "from matplotlib import transforms as mtransforms\n",
    "from matplotlib.ticker import Formatter, FixedLocator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "class SineLatitudeScale(mscale.ScaleBase):\n",
    "\n",
    "    name = 'sinelat'\n",
    "\n",
    "    def __init__(self, axis):\n",
    "        mscale.ScaleBase.__init__(self, axis)\n",
    "\n",
    "    def get_transform(self):\n",
    "        return self.SineLatitudeTransform()\n",
    "\n",
    "    def set_default_locators_and_formatters(self, axis):\n",
    "        class DegreeFormatter(Formatter):\n",
    "            def __call__(self, x, pos=None):\n",
    "                return \"%d\\N{DEGREE SIGN}\" % x\n",
    "\n",
    "        axis.set_major_locator(FixedLocator(np.arange(-90, 91, 30)))\n",
    "        axis.set_major_formatter(DegreeFormatter())\n",
    "        axis.set_minor_formatter(DegreeFormatter())\n",
    "\n",
    "    class SineLatitudeTransform(mtransforms.Transform):\n",
    "        input_dims = 1\n",
    "        output_dims = 1\n",
    "        is_separable = True\n",
    "        has_inverse = True\n",
    "\n",
    "        def __init__(self):\n",
    "            mtransforms.Transform.__init__(self)\n",
    "\n",
    "        def transform_non_affine(self, a):\n",
    "            return np.sin(np.deg2rad(a))\n",
    "\n",
    "        def inverted(self):\n",
    "            return SineLatitudeScale.InvertedSineLatitudeTransform()\n",
    "\n",
    "    class InvertedSineLatitudeTransform(mtransforms.Transform):\n",
    "        input_dims = 1\n",
    "        output_dims = 1\n",
    "        is_separable = True\n",
    "        has_inverse = True\n",
    "\n",
    "        def __init__(self):\n",
    "            mtransforms.Transform.__init__(self)\n",
    "\n",
    "        def transform_non_affine(self, a):\n",
    "            return np.rad2deg(np.arcsin(a))\n",
    "\n",
    "        def inverted(self):\n",
    "            return SineLatitudeScale.SineLatitudeTransform()\n",
    "\n",
    "mscale.register_scale(SineLatitudeScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this scale with any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax0 = plt.subplot2grid((1, 5), (0, 0))\n",
    "ax1 = plt.subplot2grid((1, 5), (0, 1), colspan=4)\n",
    "\n",
    "gpcp_dry_norm.plot(y='latitude', ax=ax0, lw=2)\n",
    "ax0.set_yscale('sinelat')\n",
    "ax0.set_ylim([-90, 90])\n",
    "ax0.grid(True)\n",
    "ax0.set_title(None)\n",
    "\n",
    "gpcp_hist_norm.plot.contourf(ax=ax1, levels=np.arange(0, 26, 2.5),\n",
    "                             cmap='Blues', xscale='log')\n",
    "ax1.set_yscale('sinelat')\n",
    "ax1.set_ylim([-90, 90])\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_ylabel(None)\n",
    "ax1.grid(True)\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "ax1.xaxis.set_major_formatter(ScalarFormatter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpcp_histogram](https://climatedataguide.ucar.edu/sites/default/files/styles/node_key_figures_display/public/key_figures/climate_data_set/cdgbutterflygpcp.jpg?itok=RnJ_w-ge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The core computation, the histogram calculation, was one line of python code. Because we used a metadata-aware data container (xarray) and a metadata-aware computational package (xhistogram), we could write code like\n",
    "\n",
    "    gpcp_hist_lon = histogram(gpcp.precip, bins=[bins], dim=['longitude']).mean(dim='time')\n",
    "    \n",
    "This is drastically simpler and easier to read than the low-level approach. Furthermore, the calculation used dask under the hood to read the data in from cloud storage and execute computations in parallel. The same code would work with a huge CMIP6 ensemble dataset.\n",
    "\n",
    "When it came time to plot, we could also use very few lines of code, since the xarray objects are aware of their their metadata and can use them to generate axis labels and other annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
